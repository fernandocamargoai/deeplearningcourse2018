{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lista de Exercícios 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from typing import List, Callable, Tuple\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sabe-se que cada neurônio, de forma individual, estabelecerá um hiperplano de separação em um dado problema. Se uma quantidade relativamente grande de neurônios forem colocados em paralelo em uma única camada, tal rede seria capaz de separar problemas não-lineares? Justifique sua resposta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ligarmos essa camada a uma camada de saída, sim. Se uma quantidade grande de neurônios forem colocados em paralelo em uma camada oculta, ligados à camada de saída, eles gerarão inúmeros hiperplanos que serão combinados pela camada de entrada. O hiperplano resultado teria um efeito serrilhado, mas seria capaz de separar problemas não lineares com uma certa precisão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Qual a importância da escolha da função de ativação em redes neurais artificiais?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além de inserir não linearidade, a função de ativação influencia muito no treinamento da rede. Isso porque as técnicas de treinamento utilizam gradiente descendente, que dependem da função ser diferenciável para seu funcionamento. A importância disso é tanta que uma das coisas que ajudou a criar redes mais profundas foi a adoção de outras funções de ativação além da sigmóide e tangente hiperbólica: RELU, ELU, Leaky-ELU, etc. Essas funções foram importantes para combater o vanishing gradient e exploding gradient, que aconteciam com maior força devido à natureza da sigmóide e tangente hiperbólica, que apresentam alta variância próximo do centro, mas variânica mínima nas bordas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Qual o possível ganho, em comparação a um único neurônio, que se pode obter em uma rede neural artificial com duas camadas de neurônios?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ela passa a ser capaz de resolver problemas não lineares. Além disso, poderá tratar de problemas multiclasses se sua camada de saída possuir mais de um neurônio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Quais são as dificuldades em usar uma rede neural de múltiplas camadas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dificuldade está em seu treinamento. Mesmo depois da criação do algoritmo backpropagation, ainda existe muita dificuldade em se empilhar camadas. Isso porque o backpropagation funciona com derivadas parciais da saída da rede em relação aos neurônios. Quando esses neurônios estão em camadas muito distantes da saída, esse gradiente vai ficando cada vez maior, apresentando o problema conhecido como vanishing gradient, em que o gradiente é tão pequeno que as camadas iniciais mal tem seus pesos alterados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Explique intuitivamente o que é a regra da cadeia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Regra da Cadeia indica como obter o coeficiente de variação (derivada) de uma função matemática composta por outra função. Essa regra dita que esse coeficiente de variação pode ser obtido ao calcular separadamente a derivada de cada função e multiplicar os resultados. Logo, pode-se dizer que o coeficiente de variação de uma função é dado pela multiplicação dos coeficientes de cada função de forma recursiva."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Explique matematicamente o que é a regra da cadeia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "Dadas as funções $f(x)$ e $g(x)$, e considerando que $f(x)$ seja composta por $g(x)$ (ex: $f(x) = g(x)^2$), a regra da cadeia indica que a derivada de $f(x)$ é dada por:"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "Dadas as funções $f(x)$ e $g(x)$, e considerando que $f(x)$ seja composta por $g(x)$ (ex: $f(x) = g(x)^2$), a regra da cadeia indica que a derivada de $f(x)$ é dada por:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$f'(x) = f'(g(x))g'(x)$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$f'(x) = f'(g(x))g'(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou melhor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\frac{df}{dx} = \\frac{df}{dg} \\frac{dg}{dx}$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex\n",
    "$\\frac{df}{dx} = \\frac{df}{dg} \\frac{dg}{dx}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caso a composição seja de múltiplas funções, essa regra se aplica recursivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Qual a importância das derivadas parciais para o processo de treinamento de uma rede neural artificial multicamadas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As derivadas parciais permitem encontrar a variação de uma função com multiplas variáveis em relação a uma delas. Dessa forma, as derivadas parciais são utilizadas para derivar a função de erro em relação a cada um dos pesos, permitindo que seja encontrada a direção de alteração de cada peso para se minimizar o erro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Quanto mais camadas uma rede neural artificial possuir, melhor o seu desempenho?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Não necessariamente. Primeiro porque uma rede profunda é mais difícil de treinar, precisando de uma quantidade maior de dados para um treinamento efetivo. Segundo que quanto maior a complexidade do modelo, mais suscetível ele está ao overfitting. Ou seja, uma rede muito profunda poderá até aprender os dados de treinamento, chegando até ao ponto de \"decorá-los\", mas não conseguirá criar uma boa generalização. Isso fará com que novos dados que a rede nunca viu sejam classificados com uma baixa precisão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Dados quatro problemas diferentes, ilustrados na figura abaixo. Como você projetaria a arquitetura de uma rede neural artificial de modo a economizar neurônios utilizados na rede?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Esse problema é linearmente separável por um único hiperplano, o que permitiria que um único neurônio fosse capaz de resolver o problema.\n",
    "\n",
    "b) Esse problema pode ser separado por múltiplos hiperplanos lineares. Então, seria necessário pelo menos um neurônio por hiperplano e uma camada de saída com um neurônio por classe. Essa camada de saída combinaria os hiperplanos e cada neurônio de saída indicaria a porcentagem de chance da entrada fazer parte de sua classe. Poderíamos, então, classificar a entrada com o neurônio de maior saída. Com essa separação, é provável que fosse necessário 5 neurônios na camada oculta (um para cada hiperplano) e 5 na camada de saída (um para cada classe).\n",
    "\n",
    "c) Esse problema é linearmente separável por um único hiperplano, o que permitiria que um único neurônio fosse capaz de resolver o problema.\n",
    "\n",
    "d) Esse problema já apresenta regiões convexas para separar as classes. Logo, seria necessário ou uma quantidade maior de neurônios numa única camada oculta, para que vários hiperplanos sejam traçados e combinados na camada de saída, ou duas camadas ocultas, o que tornaria capaz de produzir regiões convexas de separação. A quantidade de neurônios na camada de saída continua sendo 5, como na (b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Qual a relação entre gradiente descendente, gradiente descendente estocástico e mínimos locais e globais?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O Gradiente Descendente é uma técnica de otimização que visa encontrar um valor mínimo (ou máximo) de uma função desconhecida através do cálculo da derivada do erro com base nos exemplos de entrada e saída (dataset). O problema dessa técnica é que ela tende a ficar presa em mínimos locais, visto que a derivada tende ao zero quando um é alcançado. Para evitar que isso aconteça, utiliza-se o Gradiente Descendente Estocástico. Ao invés de considerar todos os exemplos de entrada e saída para se calcular a derivada, utiliza-se apenas um exemplo por vez, selecionado em ordem aleatória. Dessa forma, cada exemplo representa uma função diferente, a qual terá mínimos diferentes. Isso evita que a otimização fique presa em mínimos locais, mas deixa a otimização muito mais lenta, podendo até mesmo não convergir. Por isso, foi criada a otimização com Mini Batch. Ela utiliza de estocasticidade, mas calcula o gradiente com base em um batch de exemplos. Isso deixa a otimização mais estável e mais rápida, enquanto evita os mínimos locais. Mas cria um novo hiperparâmetro: o tamanho do mini batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Implemente o algoritmo backpropagation em linguagem python e apresente um notebook correspondente com os comentários. Por fim, mostre o gráfico de erro em relação às épocas para o conjunto de dados do exercício anterior;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(a: np.ndarray, b: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "\n",
    "def sigmoid(x: float) -> float:\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x: float) -> float:\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "class MultilayerPerceptron:\n",
    "    def __init__(self, learning_rate: float, initial_weights: List[np.ndarray] = None, initial_bias: List[float] = None,\n",
    "                 n_neurons_per_layer: List[float] = None,\n",
    "                 activation_function: Callable[[float], float] = sigmoid,\n",
    "                 derived_activation_function: Callable[[float], float] = sigmoid_derivative,\n",
    "                 batch_size=None, n_epochs=100):\n",
    "        if n_neurons_per_layer is not None:\n",
    "            self._weights: List[np.ndarray] = []\n",
    "            self._bias: List[float] = []\n",
    "            for n, m in zip(n_neurons_per_layer[0:-1], n_neurons_per_layer[1:]):\n",
    "                self._weights.append(np.random.randn(m, n).astype(np.float32) * np.sqrt(2.0/(m)))\n",
    "                self._bias.append(0)\n",
    "        else:\n",
    "            if len(initial_bias) != len(initial_weights):\n",
    "                raise ValueError(\n",
    "                    \"Deve haver um bias e uma lista de pesos por camada, sendo que ambos tenham o mesmo tamanho.\")\n",
    "            self._weights: List[np.ndarray] = initial_weights\n",
    "            self._bias: List[float] = initial_bias\n",
    "\n",
    "        self._learning_rate: float = learning_rate\n",
    "        self._activation_function: Callable[[np.ndarray], np.ndarray] = np.vectorize(activation_function)\n",
    "        self._derived_activation_function: Callable[[np.ndarray], np.ndarray] = np.vectorize(\n",
    "            derived_activation_function)\n",
    "        self._batch_size = batch_size\n",
    "        self._n_epochs = n_epochs\n",
    "\n",
    "    def _prepend_bias(self, x: np.ndarray) -> np.ndarray:\n",
    "        return np.insert(x, 0, values=1, axis=0)\n",
    "\n",
    "    def _feed_forward(self, x: np.ndarray) -> Tuple[List[np.ndarray], List[np.ndarray]]:\n",
    "        layer_input = self._prepend_bias(x)\n",
    "        layer_outputs = []\n",
    "        layer_inner_outputs = []  # Antes da função de ativação\n",
    "        for layer_weights, layer_bias in zip(self._weights, self._bias):\n",
    "            # Adiciona o bias\n",
    "            layer_weights_with_bias = np.insert(layer_weights, 0, values=layer_bias, axis=1)\n",
    "            # Calcula saída dos neurônios antes da função de ativação\n",
    "            layer_inner_output = (layer_input * layer_weights_with_bias).sum(axis=1)\n",
    "            layer_inner_outputs.append(layer_inner_output)\n",
    "            # Aplica a função de ativação\n",
    "            layer_output = self._activation_function(layer_inner_output)\n",
    "            layer_outputs.append(layer_output)\n",
    "            # Passa a saída dessa camada como entrada para a próxima\n",
    "            layer_input = self._prepend_bias(layer_output)\n",
    "\n",
    "        return layer_outputs, layer_inner_outputs\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        X = np.atleast_2d(X)\n",
    "        output = []\n",
    "        for x in X:\n",
    "            output.append(self._feed_forward(x)[0][-1])\n",
    "        return np.array(output)\n",
    "\n",
    "    def _step(self, X: np.ndarray, Y: np.ndarray) -> float:\n",
    "        updated_weights: List[np.ndarray] = [np.copy(neuron_weights) for neuron_weights in self._weights]\n",
    "        updated_bias = self._bias[:]\n",
    "\n",
    "        errors = []\n",
    "\n",
    "        for x, y in zip(X, Y):\n",
    "            layer_outputs, layer_inner_outputs = self._feed_forward(x)\n",
    "            errors.append((0.5 * (y - layer_outputs[-1]) ** 2).sum())\n",
    "\n",
    "            layer_inputs = [x] + layer_outputs\n",
    "\n",
    "            delta = None\n",
    "            for i in range(len(self._bias)):\n",
    "                layer_inner_output = layer_inner_outputs[-i - 1]\n",
    "                layer_output = layer_outputs[-i - 1]\n",
    "                layer_input = layer_inputs[-i - 2]\n",
    "                input_weights = np.array(self._weights[-i - 1])\n",
    "                output_weights = np.array(self._weights[-i]) if i > 0 else None\n",
    "\n",
    "                # Derivada do erro pela saída (após função de ativação)\n",
    "                error_derived_by_layer_output = layer_output - y if i == 0 else (output_weights.T * delta).sum(axis=1)\n",
    "                # Derivada da saída (após função de ativação) pela saída (antes da função de ativação)\n",
    "                layer_output_derived_by_layer_inner_output = self._derived_activation_function(layer_inner_output)\n",
    "                delta = error_derived_by_layer_output * layer_output_derived_by_layer_inner_output\n",
    "                # Derivada da saída (antes da função de ativação) pelo peso\n",
    "                layer_inner_output_derived_by_weight = (np.ones(input_weights.shape) * layer_input).T\n",
    "                # Derivada da saída (antes da função de ativação) pelo bias\n",
    "                layer_inner_output_derived_by_bias = 1\n",
    "\n",
    "                error_derived_by_weights = (delta * layer_inner_output_derived_by_weight).T\n",
    "                error_derived_by_bias = (delta * layer_inner_output_derived_by_bias).sum()\n",
    "\n",
    "                updated_weights[-i - 1] = updated_weights[-i - 1] - self._learning_rate * error_derived_by_weights\n",
    "                updated_bias[-i - 1] = updated_bias[-i - 1] - self._learning_rate * error_derived_by_bias\n",
    "\n",
    "        self._weights = updated_weights\n",
    "        self._bias = updated_bias\n",
    "\n",
    "        return sum(errors) / float(len(errors))\n",
    "\n",
    "    def fit(self, X: np.ndarray, Y: np.ndarray):\n",
    "        X = np.atleast_2d(X)\n",
    "        Y = np.atleast_2d(Y)\n",
    "\n",
    "        errors_history = []\n",
    "        for epoch in range(self._n_epochs):\n",
    "            errors = []\n",
    "            X_train, Y_train = shuffle(X, Y)\n",
    "            minibatch_size = min(self._batch_size, len(X_train)) if self._batch_size else len(X_train)\n",
    "\n",
    "            for i in range(0, X_train.shape[0], minibatch_size):\n",
    "                # Get pair of (X, y) of the current minibatch/chunk\n",
    "                X_train_mini = X_train[i:i + minibatch_size]\n",
    "                Y_train_mini = Y_train[i:i + minibatch_size]\n",
    "                errors.append(self._step(X_train_mini, Y_train_mini))\n",
    "\n",
    "            epoch_error = sum(errors) / float(len(errors))\n",
    "            errors_history.append(epoch_error)\n",
    "\n",
    "        return errors_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rodando para os dados da questão 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultilayerPerceptron(learning_rate=0.5, initial_weights=[np.array([[0.15, 0.2], [0.25, 0.3]]), np.array([[0.4, 0.45], [0.5, 0.55]])], initial_bias=[0.35, 0.6], n_epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0.05, 0.1]])\n",
    "Y = np.array([[0.01, 0.99]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f29c807d198>]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f29c812eba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(0, len(errors)), errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rodando para os dados de diagnóstico de fertilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Diagnostico_fertilidade.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>age</th>\n",
       "      <th>childish_diseases</th>\n",
       "      <th>trauma</th>\n",
       "      <th>surgical_intervention</th>\n",
       "      <th>high_fevers_last_year</th>\n",
       "      <th>alcool</th>\n",
       "      <th>smoking</th>\n",
       "      <th>sitting</th>\n",
       "      <th>diag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.88</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.31</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.38</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   season   age  childish_diseases  trauma  surgical_intervention  \\\n",
       "0   -0.33  0.69                  0       1                      1   \n",
       "1   -0.33  0.94                  1       0                      1   \n",
       "2   -0.33  0.50                  1       0                      0   \n",
       "3   -0.33  0.75                  0       1                      1   \n",
       "4   -0.33  0.67                  1       1                      0   \n",
       "\n",
       "   high_fevers_last_year  alcool  smoking  sitting diag  \n",
       "0                      0     0.8        0     0.88    N  \n",
       "1                      0     0.8        1     0.31    O  \n",
       "2                      0     1.0       -1     0.50    N  \n",
       "3                      0     1.0       -1     0.38    N  \n",
       "4                      0     0.8       -1     0.50    O  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['diag'] = df.diag.apply(lambda diag: 0 if diag == 'N' else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>age</th>\n",
       "      <th>childish_diseases</th>\n",
       "      <th>trauma</th>\n",
       "      <th>surgical_intervention</th>\n",
       "      <th>high_fevers_last_year</th>\n",
       "      <th>alcool</th>\n",
       "      <th>smoking</th>\n",
       "      <th>sitting</th>\n",
       "      <th>diag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.94</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.33</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   season   age  childish_diseases  trauma  surgical_intervention  \\\n",
       "0   -0.33  0.69                  0       1                      1   \n",
       "1   -0.33  0.94                  1       0                      1   \n",
       "2   -0.33  0.50                  1       0                      0   \n",
       "3   -0.33  0.75                  0       1                      1   \n",
       "4   -0.33  0.67                  1       1                      0   \n",
       "\n",
       "   high_fevers_last_year  alcool  smoking  sitting  diag  \n",
       "0                      0     0.8        0     0.88     0  \n",
       "1                      0     0.8        1     0.31     1  \n",
       "2                      0     1.0       -1     0.50     0  \n",
       "3                      0     1.0       -1     0.38     0  \n",
       "4                      0     0.8       -1     0.50     1  "
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['season', 'age', 'childish_diseases', 'trauma', 'surgical_intervention', 'high_fevers_last_year', 'alcool', 'smoking', 'sitting']].values\n",
    "Y = df[['diag']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 1)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neurons_per_layer = [9, 5, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultilayerPerceptron(learning_rate=0.01, n_neurons_per_layer=n_neurons_per_layer, n_epochs=100, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f29c190bd68>]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMsShb2aWIf8f6wFh0PVMUz8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f29c194c7b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(0, len(errors)), errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.15726681],\n",
       "       [ 0.11408394],\n",
       "       [ 0.12575938],\n",
       "       [ 0.12737791],\n",
       "       [ 0.12744949],\n",
       "       [ 0.12707081],\n",
       "       [ 0.20047518],\n",
       "       [ 0.14085338],\n",
       "       [ 0.18395236],\n",
       "       [ 0.15704884],\n",
       "       [ 0.21561723],\n",
       "       [ 0.15707426],\n",
       "       [ 0.13870618],\n",
       "       [ 0.16058761],\n",
       "       [ 0.20210959],\n",
       "       [ 0.12898195],\n",
       "       [ 0.16576285],\n",
       "       [ 0.17536265],\n",
       "       [ 0.12535082],\n",
       "       [ 0.15370238],\n",
       "       [ 0.18292516],\n",
       "       [ 0.17328538],\n",
       "       [ 0.15980426],\n",
       "       [ 0.21751107],\n",
       "       [ 0.18298547],\n",
       "       [ 0.15563573],\n",
       "       [ 0.20338938],\n",
       "       [ 0.13416872],\n",
       "       [ 0.16350924],\n",
       "       [ 0.22162606],\n",
       "       [ 0.18165989],\n",
       "       [ 0.17062224],\n",
       "       [ 0.19183634],\n",
       "       [ 0.16382769],\n",
       "       [ 0.16533757],\n",
       "       [ 0.08658692],\n",
       "       [ 0.1024614 ],\n",
       "       [ 0.12822454],\n",
       "       [ 0.14082392],\n",
       "       [ 0.09848696],\n",
       "       [ 0.09778803],\n",
       "       [ 0.10211888],\n",
       "       [ 0.13760338],\n",
       "       [ 0.10518785],\n",
       "       [ 0.07621227],\n",
       "       [ 0.08679199],\n",
       "       [ 0.13220317],\n",
       "       [ 0.11909295],\n",
       "       [ 0.12785759],\n",
       "       [ 0.13324854],\n",
       "       [ 0.12645569],\n",
       "       [ 0.09438413],\n",
       "       [ 0.11737509],\n",
       "       [ 0.14923552],\n",
       "       [ 0.14096249],\n",
       "       [ 0.11882087],\n",
       "       [ 0.12978801],\n",
       "       [ 0.18902505],\n",
       "       [ 0.15105435],\n",
       "       [ 0.12923894],\n",
       "       [ 0.17369864],\n",
       "       [ 0.09980786],\n",
       "       [ 0.13767343],\n",
       "       [ 0.08387317],\n",
       "       [ 0.08670574],\n",
       "       [ 0.15099881],\n",
       "       [ 0.19517021],\n",
       "       [ 0.09617329],\n",
       "       [ 0.10293377],\n",
       "       [ 0.20632505],\n",
       "       [ 0.19517021],\n",
       "       [ 0.10373498],\n",
       "       [ 0.13726951],\n",
       "       [ 0.08988078],\n",
       "       [ 0.09554298],\n",
       "       [ 0.0875994 ],\n",
       "       [ 0.09346396],\n",
       "       [ 0.07865696],\n",
       "       [ 0.08056313],\n",
       "       [ 0.08475449],\n",
       "       [ 0.08705635],\n",
       "       [ 0.0724595 ],\n",
       "       [ 0.10259096],\n",
       "       [ 0.0791985 ],\n",
       "       [ 0.10510809],\n",
       "       [ 0.11701192],\n",
       "       [ 0.11407372],\n",
       "       [ 0.09037176],\n",
       "       [ 0.11144033],\n",
       "       [ 0.09723126],\n",
       "       [ 0.09289663],\n",
       "       [ 0.12490141],\n",
       "       [ 0.13904444],\n",
       "       [ 0.16026461],\n",
       "       [ 0.18039787],\n",
       "       [ 0.10707171],\n",
       "       [ 0.10763425],\n",
       "       [ 0.10453331],\n",
       "       [ 0.08792334],\n",
       "       [ 0.13910053]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

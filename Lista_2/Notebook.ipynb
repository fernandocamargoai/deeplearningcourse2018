{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lista de Exercícios 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sabe-se que cada neurônio, de forma individual, estabelecerá um hiperplano de separação em um dado problema. Se uma quantidade relativamente grande de neurônios forem colocados em paralelo em uma única camada, tal rede seria capaz de separar problemas não-lineares? Justifique sua resposta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ligarmos essa camada a uma camada de saída, sim. Se uma quantidade grande de neurônios forem colocados em paralelo em uma camada oculta, ligados à camada de saída, eles gerarão inúmeros hiperplanos que serão combinados pela camada de entrada. O hiperplano resultado teria um efeito serrilhado, mas seria capaz de separar problemas não lineares com uma certa precisão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Qual a importância da escolha da função de ativação em redes neurais artificiais?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Além de inserir não linearidade, a função de ativação influencia muito no treinamento da rede. Isso porque as técnicas de treinamento utilizam gradiente descendente, que dependem da função ser diferenciável para seu funcionamento. A importância disso é tanta que uma das coisas que ajudou a criar redes mais profundas foi a adoção de outras funções de ativação além da sigmóide e tangente hiperbólica: RELU, ELU, Leaky-ELU, etc. Essas funções foram importantes para combater o vanishing gradient e exploding gradient, que aconteciam com maior força devido à natureza da sigmóide e tangente hiperbólica, que apresentam alta variância próximo do centro, mas variânica mínima nas bordas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Qual o possível ganho, em comparação a um único neurônio, que se pode obter em uma rede neural artificial com duas camadas de neurônios?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ela passa a ser capaz de resolver problemas não lineares. Além disso, poderá tratar de problemas multiclasses se sua camada de saída possuir mais de um neurônio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Quais são as dificuldades em usar uma rede neural de múltiplas camadas?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dificuldade está em seu treinamento. Mesmo depois da criação do algoritmo backpropagation, ainda existe muita dificuldade em se empilhar camadas. Isso porque o backpropagation funciona com derivadas parciais da saída da rede em relação aos neurônios. Quando esses neurônios estão em camadas muito distantes da saída, esse gradiente vai ficando cada vez maior, apresentando o problema conhecido como vanishing gradient, em que o gradiente é tão pequeno que as camadas iniciais mal tem seus pesos alterados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
